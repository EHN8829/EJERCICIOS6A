# -*- coding: utf-8 -*-
"""DUPD_FINAL2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tRA3LMBMqOOEUzqVaY0Bi_vhVdspM-9G

---
$$\small\textbf{Análisis de sensaciones a videos de Ecuaciones Diferenciales en YouTube como material de estudio complementario}$$

---
<br>

$\small\text{Autor: Eginhardo Navarro Honda}$

---

$$\large\textbf{Método 2}$$

---

$\small\text{1. Instalando las librerías necesarias}$
"""

!pip install streamlit

from googleapiclient.discovery import build
import getpass

"""$\small\text{2. API Key}$"""

API_KEY = getpass.getpass('Ingrese su YouTube API Key: ')
playlist_ids = ['AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30']

# Configurando el API Key
youtube = build('youtube', 'v3', developerKey=API_KEY)

"""$\small\text{3. Obtención de los ID's de los videos del Playlist}$"""

def get_all_video_ids_from_playlists(youtube, playlist_ids):
    all_videos = []  # Inicializando una única lista para almacenar todos los ID's de los videos

    for playlist_id in playlist_ids:
        next_page_token = None

# Obteniendo los videos de la lista de reproducción actual
def get_all_video_ids_from_playlists(youtube, playlist_ids):
    all_videos = []  # Inicializando una única lista para almacenar todos los ID's de los videos

    for playlist_id in playlist_ids:
        next_page_token = None

        # Obteniendo los videos de la lista de reproducción actual
        while True:
            playlist_request = youtube.playlistItems().list(
                part='contentDetails',
                playlistId=playlist_id,
                maxResults=25, # Se puede modificar el máximo número de resultados
                pageToken=next_page_token
            )
            playlist_response = playlist_request.execute()

            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]

            next_page_token = playlist_response.get('nextPageToken')

            if next_page_token is None:
                break

    return all_videos # Se corrigió la sangría de esta línea para que coincida con la definición de la función (Sugerencia de GEMINI)

# Enlistando ID's de listas de reproducción
playlist_ids = ['PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A']  # Se puede añadir más ID's si estimamos conveniente y/o necesario
video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)

video_ids

"""$\small\text{4. Librerías necesarias para obtener el total de comentarios}$"""

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from oauth2client.tools import argparser

# Inicializando la API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Obteniendo comentarios para cada video
def get_video_comments(youtube, video_id, max_comments=25): # Establecemos solamente 25 comentarios para cada video
    comments = []
    next_page_token = None

    while True:
        try:
            # Llamando al API para conseguir los comentarios
            response = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Leyendo cada comentario y añadiendo a la lista
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments.append(comment)

            # Comprobando la existencia de más páginas con comentarios
            next_page_token = response.get('nextPageToken')
            if next_page_token is None or len(comments) >= max_comments:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return comments[:max_comments]

# Definiendo la función principal
def main():
    # Iniciando la API de YouTube
    youtube = initialize_youtube()

    # Leyendo cada ID de video y obteniendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_video_comments(youtube, video_id)

        # Imprimiendo los comentarios obtenidos
        for idx, comment in enumerate(comments, start=1):
            print(f'Comentario {idx}: {comment}')
        print()

if __name__ == '__main__':
    main()

"""$\small\text{5. Registro de tiempo (Timestamp), Nombre de usuario (Username), Enlace de video (VideoID), Comentario (Comment) y Fecha (Date)}$"""

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Adquiriendo todos los comentarios de un video
def get_all_video_comments(youtube, video_id):
    comments = []
    next_page_token = None

    while True:
        try:
            # Convocando al API para extraer los comentarios
            response = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada comentario y agregando a la lista
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments.append(comment)

            # Comprobando páginas adicionales de comentarios
            next_page_token = response.get('nextPageToken')
            if next_page_token is None:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return comments

# Creando el Dataframe con todos los comentarios de algún video
def create_comments_dataframe(video_id, comments):
    df = pd.DataFrame(comments, columns=['Comment'])
    df['VideoId'] = video_id
    df['VideoUrl'] = f'https://www.youtube.com/watch?v={video_id}'
    return df

# Función principal
def main():

    # Inicializando el API de YouTube
    youtube = initialize_youtube()

# Listando todos dataframes de comentarios para su almacenamiento
    all_dfs = []

# Navegando en cada ID de video y extrayendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_all_video_comments(youtube, video_id)
        df = create_comments_dataframe(video_id, comments)
        all_dfs.append(df)

    # Concatenando todos los dataframes en uno
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Guardando el dataframe en un archivo CSV
    final_df.to_csv('youtube_comments.csv', index=False)

    print('Proceso completado. Se ha creado el archivo youtube_comments.csv')

if __name__ == '__main__':
    main()

!ls

"""$\small\text{5.1. Descarga del archivo [youtube_comments.csv] en el procesador}$"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download("youtube_comments.csv")

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Enlistando ID's de listas de reproducción
playlist_ids = ['PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A']  # Se puede añadir más ID's si estimamos conveniente y/o necesario
video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)

video_ids

# Listando de ID's de video de YouTube
video_ids = [
 'rd2jKGQJucE',
 '_GW79tPHmVA',
 'NYE1GhhiF7E',
 'j1nwW8eyD0Q',
 '2S1XmMzFUe0',
 'WAYBZTQoVkI',
 'veSNESx8XBk',
 'jA8nVFz94eY',
 'A41Xtv_tatE',
 'kufA4JGm_sU',
 '0pR_tZAVnUE',
 '1_kJsHwf64c',
 '2Rq34fbl1J4',
 'Ksw0yqOK00I',
 'v3smSegJR50',
 'MyH8ItOpkjA',
 '-RHTuM3hNII',
 'UVMtTO5H1bk',
 'ejyLvEIpv-Q',
 'yI1vvwPKB1c',
 'h8JY9daqCBY',
 'kqzOGLstWh8',
 'SucuT1pw8Jc',
 'QBrcu1kBd7k',
 'lr9ToJnwPEk',
 'E2g4UffqKw0'
]

# Inicializando el API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Consiguiendo todos los comentarios (incluidas respuestas) de algún video
def get_comments_for_video(youtube, video_id):
    all_comments = []
    next_page_token = None

    while True:
        try:
            # Convocando al API para extraer los comentarios
            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100
            ).execute()

            # Navegando en cada comentario y agregándolo a una lista
            for item in response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'Timestamp': top_comment['publishedAt'],
                    'Username': top_comment['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': top_comment['textDisplay'],
                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']
                }
                all_comments.append(comment_data)

                # Obteniendo las respuestas si existe alguna
                if item['snippet']['totalReplyCount'] > 0:
                    all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return all_comments

# Extrayendo las respuestas a los comentarios
def get_replies(youtube, parent_id, video_id):
    replies = []
    next_page_token = None

    while True:
        try:
            # Convocando al API para extraer las respuestas
            response = youtube.comments().list(
                part='snippet',
                parentId=parent_id,
                textFormat='plainText',
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada respuesta y añadiéndola a la lista
            for item in response['items']:
                reply_data = {
                    'Timestamp': item['snippet']['publishedAt'],
                    'Username': item['snippet']['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': item['snippet']['textDisplay'],
                    'Date': item['snippet']['updatedAt'] if 'updatedAt' in item['snippet'] else item['snippet']['publishedAt']
                }
                replies.append(reply_data)

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return replies

# Creando un dataframe con todos los comentarios de algún video
def create_comments_dataframe(video_id, comments):
    df = pd.DataFrame(comments)
    df['VideoId'] = video_id
    df['VideoUrl'] = f'https://www.youtube.com/watch?v={video_id}'
    return df

# Función principal
def main():
    # Inicializando el API de YouTube
    youtube = initialize_youtube()

    # Listando todos los dataframes de comentarios para su almacenamiento
    all_dfs = []

    # Navegando en cada ID de video y obteniendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_comments_for_video(youtube, video_id)
        df = create_comments_dataframe(video_id, comments)
        all_dfs.append(df)

    # Concatenando todos los dataframes a uno solo
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Guardando el dataframe en un archivo CSV
    final_df.to_csv('youtube_comments_with_info.csv', index=False)

    print('Proceso completado. Se ha creado el archivo youtube_comments_with_info.csv')

if __name__ == '__main__':
    main()

"""$\small\text{5.2. Descarga del archivo [youtube_comments_with_info.csv] en el procesador}$"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download("youtube_comments_with_info.csv")

"""$\small\text{6. Modificación en la cantidad de likes y cantidad de respuestas que recibió algún comentario}$"""

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Configurando la clave del API Key en la consola para desarrolladores de Google
API_KEY = 'AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30'

# Listando de ID's de video de YouTube
video_ids = [
 'rd2jKGQJucE',
 '_GW79tPHmVA',
 'NYE1GhhiF7E',
 'j1nwW8eyD0Q',
 '2S1XmMzFUe0',
 'WAYBZTQoVkI',
 'veSNESx8XBk',
 'jA8nVFz94eY',
 'A41Xtv_tatE',
 'kufA4JGm_sU',
 '0pR_tZAVnUE',
 '1_kJsHwf64c',
 '2Rq34fbl1J4',
 'Ksw0yqOK00I',
 'v3smSegJR50',
 'MyH8ItOpkjA',
 '-RHTuM3hNII',
 'UVMtTO5H1bk',
 'ejyLvEIpv-Q',
 'yI1vvwPKB1c',
 'h8JY9daqCBY',
 'kqzOGLstWh8',
 'SucuT1pw8Jc',
 'QBrcu1kBd7k',
 'lr9ToJnwPEk',
 'E2g4UffqKw0'
]

# Inicializando el API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Extrayendo los comentarios (incluidas respuestas) del video
def get_comments_for_video(youtube, video_id):
    all_comments = []
    next_page_token = None

    while True:
        try:
            # Convocando el API para extraer los comentarios
            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100
            ).execute()

            # Navegando en cada comentario y añadiendo a una lista
            for item in response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'Timestamp': top_comment['publishedAt'],
                    'Username': top_comment['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': top_comment['textDisplay'],
                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt'],
                    'Likes': top_comment['likeCount'],
                    'Replies': item['snippet']['totalReplyCount']
                }
                all_comments.append(comment_data)

                # Obteniendo respuestas (si existe algunas)
                if item['snippet']['totalReplyCount'] > 0:
                    all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return all_comments

# Extrayendo las respuestas a los comentarios
def get_replies(youtube, parent_id, video_id):
    replies = []
    next_page_token = None

    while True:
        try:
            # Llamando el API para obtener respuestas
            response = youtube.comments().list(
                part='snippet',
                parentId=parent_id,
                textFormat='plainText',
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada respuesta y añadiendo a una lista
            for item in response['items']:
                reply_data = {
                    'Timestamp': item['snippet']['publishedAt'],
                    'Username': item['snippet']['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': item['snippet']['textDisplay'],
                    'Date': item['snippet']['updatedAt'] if 'updatedAt' in item['snippet'] else item['snippet']['publishedAt'],
                    'Likes': item['snippet']['likeCount'],
                    'Replies': 0  # Las respuestas a respuestas no se toman en cuenta como etiquetas (no están etiquetados)
                }
                replies.append(reply_data)

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return replies

# Creando un Dataframe con todos los comentarios de algún video
def create_comments_dataframe(video_id, comments):
    df = pd.DataFrame(comments)
    df['VideoId'] = video_id
    df['VideoUrl'] = f'https://www.youtube.com/watch?v={video_id}'
    return df

# Función principal
def main():

    # Inicializando el API de YouTube
    youtube = initialize_youtube()

    # Listando los comentarios en Dataframes
    all_dfs = []

    # Navegando en cada ID del video y extrayendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_comments_for_video(youtube, video_id)
        df = create_comments_dataframe(video_id, comments)
        all_dfs.append(df)

    # Concatenando los Dataframes en un único DF
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Guardando el dataframe en un archivo CSV
    final_df.to_csv('youtube_comments_with_likes_replies.csv', index=False)

    print('Proceso completado. Se ha creado el archivo youtube_comments_with_likes_replies.csv')

if __name__ == '__main__':
    main()

"""$\small\text{6.1. Descarga del archivo [youtube_comments_with_likes_replies.csv] en el procesador}$"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download("youtube_comments_with_likes_replies.csv")

"""$\small\text{7. Creación de un Dataframe de los comentarios de los usuarios y gráfico de un diagrama de barras (indicando la frecuencia del top 10)}$"""

import pandas as pd
import matplotlib.pyplot as plt
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Listando de ID's de video de YouTube
video_ids = [
 'rd2jKGQJucE',
 '_GW79tPHmVA',
 'NYE1GhhiF7E',
 'j1nwW8eyD0Q',
 '2S1XmMzFUe0',
 'WAYBZTQoVkI',
 'veSNESx8XBk',
 'jA8nVFz94eY',
 'A41Xtv_tatE',
 'kufA4JGm_sU',
 '0pR_tZAVnUE',
 '1_kJsHwf64c',
 '2Rq34fbl1J4',
 'Ksw0yqOK00I',
 'v3smSegJR50',
 'MyH8ItOpkjA',
 '-RHTuM3hNII',
 'UVMtTO5H1bk',
 'ejyLvEIpv-Q',
 'yI1vvwPKB1c',
 'h8JY9daqCBY',
 'kqzOGLstWh8',
 'SucuT1pw8Jc',
 'QBrcu1kBd7k',
 'lr9ToJnwPEk',
 'E2g4UffqKw0'
]

# Inicializando el API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Obteniendo los comentarios (incluidas respuestas)
def get_comments_for_video(youtube, video_id):
    all_comments = []
    next_page_token = None

    while True:
        try:
            # Convocando el API para extraer los comentarios
            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100
            ).execute()

            # Navegando en cada comentario y añadiendo a una lista
            for item in response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'Timestamp': top_comment['publishedAt'],
                    'Username': top_comment['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': top_comment['textDisplay'],
                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt'],
                    'Likes': top_comment['likeCount'],
                    'Replies': item['snippet']['totalReplyCount']
                }
                all_comments.append(comment_data)

                # Obteniendo las respuestas (si existe alguna)
                if item['snippet']['totalReplyCount'] > 0:
                    all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return all_comments

# Obteniendo las respuestas a los comentarios
def get_replies(youtube, parent_id, video_id):
    replies = []
    next_page_token = None

    while True:
        try:
            # Emplazando el API para extraer las respuestas
            response = youtube.comments().list(
                part='snippet',
                parentId=parent_id,
                textFormat='plainText',
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada respuesta y añadiendo a una lista
            for item in response['items']:
                reply_data = {
                    'Timestamp': item['snippet']['publishedAt'],
                    'Username': item['snippet']['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': item['snippet']['textDisplay'],
                    'Date': item['snippet']['updatedAt'] if 'updatedAt' in item['snippet'] else item['snippet']['publishedAt'],
                    'Likes': item['snippet']['likeCount'],
                    'Replies': 0  # Las respuestas a respuestas no se cuentan como etiquetas
                }
                replies.append(reply_data)

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return replies

# Obteniendo los comentarios de los usuarios para un gráfico de frecuencias (top 10)
def main():

    # Inicializando el API de YouTube
    youtube = initialize_youtube()

    # Listando los Dataframes de comentarios para su almacenamiento
    all_dfs = []

    # Navegando en cada ID de un video y extrayendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_comments_for_video(youtube, video_id)
        df = pd.DataFrame(comments)
        all_dfs.append(df)

    # Concatenando todos los Dataframes en un único DF
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Adquiriendo listado de los usuarios que comentaron
    user_counts = final_df['Username'].value_counts().head(10)

    # Creando el gráfico de barras
    plt.figure(figsize=(10, 6))
    user_counts.plot(kind='bar', color='red')
    plt.title('Top 10 Usuarios que Comentaron en los Videos')
    plt.xlabel('Usuarios')
    plt.ylabel('Frecuencia')
    plt.xticks(rotation=90, ha='right')
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()

"""$\small\text{8. Análisis de Sensaciones}$"""

!pip install textblob

!ls

import pandas as pd
from textblob import TextBlob
import seaborn as sns

# Importando el archivo de sensaciones desde GitHub
url = 'https://raw.githubusercontent.com/EHN8829/DUPD_FINAL/main/youtube_comments.csv'
df = pd.read_csv(url)

# Mostrando las primeras filas del DataFrame
print(df.head(25))

df.head(50)

df['polaridad']=df['Comment'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['subjetividad']=df['Comment'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

sns.displot(df['polaridad'],color='navy')
sns.displot(df['subjetividad'],color='lightblue')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Creando un gráfico superpuesto
plt.figure(figsize=(10, 6))  # Ajuste del tamaño del gráfico (según el caso)

# Graficando la polaridad en rojo oscuro
sns.histplot(df['polaridad'], color='crimson', kde=True, label='Polaridad')

# Graficando la subjetividad en verde claro
sns.histplot(df['subjetividad'], color='cornsilk', kde=True, label='Subjetividad')

# Ajustando las etiquetas y leyenda
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.title('Distribución de Polaridad y Subjetividad')
plt.legend()

# Mostrando el gráfico
plt.show()

"""$\text{- Se observa que los valores para la subjetividad [cornsilk] y polaridad [crimson] son mayores a cero, respectivamente.}$
$\text{- Significa que los comentarios son objetivos respecto a la subjetividad y respecto a la polaridad existe comentarios positivos y negativos.}$
$\text{- Predomina una mayor cantidad de comentarios neutros, como una mínima cantidad de positivos. Pero, mayor respecto a los comentarios negativos.}$

$\small\text{9. Adicional: Nube de tags (palabras más significativas)}$
"""

from wordcloud import WordCloud

text = ' '.join(df['Comment'])
text

wordcloud = WordCloud(max_font_size=60, max_words=200, background_color="azure").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")

wordcloud = WordCloud(width=1024, height=800, colormap="Reds", min_font_size=14).generate(text)

plt.figure(figsize=(12, 10), facecolor=None)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()