# -*- coding: utf-8 -*-
"""DUPD_FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/EHN8829/DUPD_FINAL/blob/main/DUPD_FINAL.ipynb

---
$$\small\textbf{Análisis de sensaciones a videos de Ecuaciones Diferenciales en YouTube como material de estudio complementario}$$

---
<br>

$\small\text{Autor: Eginhardo Navarro Honda}$

---

$$\large\textbf{Método 1}$$

---

$\small\text{1. Instalando las librerías necesarias}$
"""

!pip install google-api-python-client
!pip install textblob
!pip install nltk

"""$\small\text{2. Configurando la API Key de YouTube}$"""

API_KEY = 'AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30'

"""$\small\text{3. Funciones para la obtención de datos en los videos}$"""

from googleapiclient.discovery import build

# Configurando la API Key
youtube = build('youtube', 'v3', developerKey=API_KEY)

def get_video_ids(playlist_id):
    video_ids = []
    request = youtube.playlistItems().list(
        part='contentDetails',
        playlistId=playlist_id,
        maxResults=50  # Puedes ajustar este número si tienes más de 50 videos
    )
    response = request.execute()
    for item in response['items']:
        video_ids.append(item['contentDetails']['videoId'])
    return video_ids

def get_comments(video_id):
    comments = []
    request = youtube.commentThreads().list(
        part='snippet',
        videoId=video_id,
        textFormat='plainText',
        maxResults=100  # Puedes ajustar este número
    )
    response = request.execute()
    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        comments.append(comment)
    return comments

# ID de la lista de reproducción
playlist_id = 'PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A'
video_ids = get_video_ids(playlist_id)

# Obteniendo los comentarios para cada video
video_comments = {}
for video_id in video_ids:
    video_comments[video_id] = get_comments(video_id)

# Mostrando los resultados
print(video_comments)

"""$\small\text{3. Analizando las sensaciones }$

$\small\text{3.1. Instalando las librerías necesarias}$
"""

from textblob import TextBlob
import nltk
nltk.download('punkt')

def analyze_sentiment(comments):
    sentiments = []
    for comment in comments:
        blob = TextBlob(comment)
        sentiments.append(blob.sentiment.polarity)  # Polaridad: De -1 (negativo) a 1 (positivo)
    return sentiments

# Analizando las sensaciones de los comentarios para cada video
video_sentiments = {}
for video_id, comments in video_comments.items():
    video_sentiments[video_id] = analyze_sentiment(comments)

# Mostrando las sensaciones
print(video_sentiments)

"""$\small\text{4. Exportando y visualizando los resultados}$"""

import pandas as pd

"""$\small\text{4.1. Verificando el contenido de video_sentiments}$"""

for video_id, sentiments in video_sentiments.items():
    print(f"Video ID: {video_id}")
    print(f"Sentiments: {sentiments[:9]}")  # Muestra solo las primeras 9 sensaciones para simplificar
    print("------")

"""$\small\text{4.2. Preparando los Datos para el DataFrame}$"""

# Calculando la sensación promedio para cada video
def calculate_average_sentiment(sentiments):
    if sentiments:
        return sum(sentiments) / len(sentiments)
    return 0

# Creando un diccionario con los promedios de la sensación
average_sentiments = {video_id: calculate_average_sentiment(sentiments) for video_id, sentiments in video_sentiments.items()}

# Convirtiendo a DataFrame
df = pd.DataFrame(list(average_sentiments.items()), columns=['VideoID', 'AverageSentiment'])

# Configurando el índice (si es necesario)
df.set_index('VideoID', inplace=True)

# Guardando la 'data' en un archivo CSV
df.to_csv('/content/video_sentiments.csv')

"""$\small\text{4.3. Descargando un archivo [video_sentiments.csv] en el procesador}$"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download('video_sentiments.csv')

# Mostrando el DataFrame
print(df)

"""$\small\text{5. Instalando y configurando las Bibliotecas de Visualización}$"""

!pip install matplotlib seaborn

"""$\small\text{6. Creando gráficos usando Matplotlib y Seaborn}$

$\small\text{6.1. Histograma de Sensaciones}$
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Configurando el estilo de los gráficos
sns.set(style="whitegrid")

# Creando histograma
plt.figure(figsize=(10, 6))
sns.histplot(df['AverageSentiment'], bins=20, kde=True, color='orange')
plt.title('Distribución de Sensaciones Promedio de Videos')
plt.xlabel('Sensación Promedio')
plt.ylabel('Frecuencia')
plt.show()

"""$\small\text{6.2. Gráfico de Barras de Sensaciones por Video}$"""

# Ordenando los valores para mejorar la visualización
df_sorted = df.sort_values(by='AverageSentiment', ascending=False)

# Creando el gráfico de barras
plt.figure(figsize=(14, 8))
sns.barplot(x=df_sorted.index, y=df_sorted['AverageSentiment'], palette='viridis')
plt.title('Sensaciones Promedio por Video')
plt.xlabel('ID del Video')
plt.ylabel('Sensación Promedio')
plt.xticks(rotation=90)  # Rotando 90° las etiquetas del eje 'x' para mayor legibilidad
plt.show()

"""$\small\text{6.3. Gráfico de Caja (Boxplot) de Sensaciones}$"""

# Creando el gráfico de caja
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['AverageSentiment'], color='cyan')
plt.title('Distribución de Sensaciones Promedio')
plt.xlabel('Sensación Promedio')
plt.show()

"""$\small\text{7. Obteniendo los Comentarios positivos para realizar un Análisis de Sensaciones}$"""

!pip install pandas textblob nltk

import pandas as pd
from textblob import TextBlob
import nltk
nltk.download('punkt')  # Descargando el tokenizador de nltk nuevamente

def analyze_sentiment(comment):
    blob = TextBlob(comment)
    return blob.sentiment.polarity  # Polaridad: -1 De (negativo) a 1 (positivo)

# Creando una lista para almacenar los comentarios positivos
positive_comments = []

# Analizando cada video y sus comentarios
for video_id, comments in video_comments.items():
    for comment in comments:
        sentiment = analyze_sentiment(comment)
        if sentiment > 0:  # Considerando como positivo si la polaridad es mayor que 0
            positive_comments.append({
                'VideoID': video_id,
                'Comment': comment,
                'Sentiment': sentiment
            })

# Convirtiendo la lista a un DataFrame
df_positive_comments = pd.DataFrame(positive_comments)

# Mostrando el DataFrame
print(df_positive_comments)

df_positive_comments

# Mostrando la tabla de comentarios positivos
plt.figure(figsize=(12, 8))
ax = sns.heatmap(df_positive_comments[['Comment', 'Sentiment']].head(20).set_index('Comment'), annot=True, cmap='YlGnBu')
ax.set_title('Comentarios Positivos')
plt.show()

"""---

$$\large\textbf{Método 2}$$

---

$\small\text{1. Librerías}$
"""

from googleapiclient.discovery import build
import getpass

"""$\small\text{2. API Key}$"""

API_KEY = getpass.getpass('Ingrese su YouTube API Key: ')
# playlist_ids = ['AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30']

# Configurando el API Key
youtube = build('youtube', 'v3', developerKey=API_KEY)

"""$\small\text{3. Obtención de los ID's de los videos del Playlist}$"""

def get_all_video_ids_from_playlists(youtube, playlist_ids):
    all_videos = []  # Inicializando una única lista para almacenar todos los ID's de los videos

    for playlist_id in playlist_ids:
        next_page_token = None

# Obteniendo los videos de la lista de reproducción actual
def get_all_video_ids_from_playlists(youtube, playlist_ids):
    all_videos = []  # Inicializando una única lista para almacenar todos los ID's de los videos

    for playlist_id in playlist_ids:
        next_page_token = None

        # Obteniendo los videos de la lista de reproducción actual
        while True:
            playlist_request = youtube.playlistItems().list(
                part='contentDetails',
                playlistId=playlist_id,
                maxResults=25, # Se puede modificar el máximo número de resultados
                pageToken=next_page_token
            )
            playlist_response = playlist_request.execute()

            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]

            next_page_token = playlist_response.get('nextPageToken')

            if next_page_token is None:
                break

    return all_videos # Se corrigió la sangría de esta línea para que coincida con la definición de la función (Sugerencia de GEMINI)

# Enlistando ID's de listas de reproducción
playlist_ids = ['PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A']  # Se puede añadir más ID's si estimamos conveniente y/o necesario
video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)

video_ids

"""$\small\text{4. Librerías necesarias para obtener el total de comentarios}$"""

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from oauth2client.tools import argparser

#API_KEY = 'AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30'

# Inicializando la API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Obteniendo comentarios para cada video
def get_video_comments(youtube, video_id, max_comments=25): # Establecemos solamente 25 comentarios para cada video
    comments = []
    next_page_token = None

    while True:
        try:
            # Llamando al API para conseguir los comentarios
            response = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Leyendo cada comentario y añadiendo a la lista
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments.append(comment)

            # Comprobando la existencia de más páginas con comentarios
            next_page_token = response.get('nextPageToken')
            if next_page_token is None or len(comments) >= max_comments:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return comments[:max_comments]

# Definiendo la función principal
def main():
    # Iniciando la API de YouTube
    youtube = initialize_youtube()

    # Leyendo cada ID de video y obteniendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_video_comments(youtube, video_id)

        # Imprimiendo los comentarios obtenidos
        for idx, comment in enumerate(comments, start=1):
            print(f'Comentario {idx}: {comment}')
        print()

if __name__ == '__main__':
    main()

"""$\small\text{5. Registro de tiempo (Timestamp), Nombre de usuario (Username), Enlace de video (VideoID), Comentario (Comment) y Fecha (Date)}$"""

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Adquiriendo todos los comentarios de un video
def get_all_video_comments(youtube, video_id):
    comments = []
    next_page_token = None

    while True:
        try:
            # Convocando al API para extraer los comentarios
            response = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada comentario y agregando a la lista
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments.append(comment)

            # Comprobando páginas adicionales de comentarios
            next_page_token = response.get('nextPageToken')
            if next_page_token is None:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return comments

# Creando el Dataframe con todos los comentarios de algún video
def create_comments_dataframe(video_id, comments):
    df = pd.DataFrame(comments, columns=['Comment'])
    df['VideoId'] = video_id
    df['VideoUrl'] = f'https://www.youtube.com/watch?v={video_id}'
    return df

# Función principal
def main():

    # Inicializando el API de YouTube
    youtube = initialize_youtube()

# Listando todos dataframes de comentarios para su almacenamiento
    all_dfs = []

# Navegando en cada ID de video y extrayendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_all_video_comments(youtube, video_id)
        df = create_comments_dataframe(video_id, comments)
        all_dfs.append(df)

    # Concatenando todos los dataframes en uno
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Guardando el dataframe en un archivo CSV
    final_df.to_csv('youtube_comments.csv', index=False)

    print('Proceso completado. Se ha creado el archivo youtube_comments.csv')

if __name__ == '__main__':
    main()

!ls

"""$\small\text{5.1. Descargando un archivo [youtube_comments.csv] en el procesador}$


"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download("youtube_comments.csv")

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Enlistando ID's de listas de reproducción
playlist_ids = ['PLeySRPnY35dFSDPi_4Q5R1VCGL_pab26A']  # Se puede añadir más ID's si estimamos conveniente y/o necesario
video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)

video_ids

# Listando de ID's de video de YouTube
video_ids = [
 'rd2jKGQJucE',
 '_GW79tPHmVA',
 'NYE1GhhiF7E',
 'j1nwW8eyD0Q',
 '2S1XmMzFUe0',
 'WAYBZTQoVkI',
 'veSNESx8XBk',
 'jA8nVFz94eY',
 'A41Xtv_tatE',
 'kufA4JGm_sU',
 '0pR_tZAVnUE',
 '1_kJsHwf64c',
 '2Rq34fbl1J4',
 'Ksw0yqOK00I',
 'v3smSegJR50',
 'MyH8ItOpkjA',
 '-RHTuM3hNII',
 'UVMtTO5H1bk',
 'ejyLvEIpv-Q',
 'yI1vvwPKB1c',
 'h8JY9daqCBY',
 'kqzOGLstWh8',
 'SucuT1pw8Jc',
 'QBrcu1kBd7k',
 'lr9ToJnwPEk',
 'E2g4UffqKw0'
]

# Inicializando el API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Consiguiendo todos los comentarios (incluidas respuestas) de algún video
def get_comments_for_video(youtube, video_id):
    all_comments = []
    next_page_token = None

    while True:
        try:
            # Convocando al API para extraer los comentarios
            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100
            ).execute()

            # Navegando en cada comentario y agregándolo a una lista
            for item in response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'Timestamp': top_comment['publishedAt'],
                    'Username': top_comment['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': top_comment['textDisplay'],
                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']
                }
                all_comments.append(comment_data)

                # Obteniendo las respuestas si existe alguna
                if item['snippet']['totalReplyCount'] > 0:
                    all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return all_comments

# Extrayendo las respuestas a los comentarios
def get_replies(youtube, parent_id, video_id):
    replies = []
    next_page_token = None

    while True:
        try:
            # Convocando al API para extraer las respuestas
            response = youtube.comments().list(
                part='snippet',
                parentId=parent_id,
                textFormat='plainText',
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada respuesta y añadiéndola a la lista
            for item in response['items']:
                reply_data = {
                    'Timestamp': item['snippet']['publishedAt'],
                    'Username': item['snippet']['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': item['snippet']['textDisplay'],
                    'Date': item['snippet']['updatedAt'] if 'updatedAt' in item['snippet'] else item['snippet']['publishedAt']
                }
                replies.append(reply_data)

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return replies

# Creando un dataframe con todos los comentarios de algún video
def create_comments_dataframe(video_id, comments):
    df = pd.DataFrame(comments)
    df['VideoId'] = video_id
    df['VideoUrl'] = f'https://www.youtube.com/watch?v={video_id}'
    return df

# Función principal
def main():
    # Inicializando el API de YouTube
    youtube = initialize_youtube()

    # Listando todos los dataframes de comentarios para su almacenamiento
    all_dfs = []

    # Navegando en cada ID de video y obteniendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_comments_for_video(youtube, video_id)
        df = create_comments_dataframe(video_id, comments)
        all_dfs.append(df)

    # Concatenando todos los dataframes a uno solo
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Guardando el dataframe en un archivo CSV
    final_df.to_csv('youtube_comments_with_info.csv', index=False)

    print('Proceso completado. Se ha creado el archivo youtube_comments_with_info.csv')

if __name__ == '__main__':
    main()

"""$\small\text{5.2. Descargando un archivo [youtube_comments_with_info.csv] en el procesador}$"""

from google.colab import files

# Activando una descarga en nuestro procesador
files.download("youtube_comments_with_info.csv")

"""$\small\text{6. Modificación en la cantidad de likes y cantidad de respuestas que recibió algún comentario}$"""

import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Configurando la clave del API Key en la consola para desarrolladores de Google
API_KEY = 'AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30'

# Listando de ID's de video de YouTube
video_ids = [
 'rd2jKGQJucE',
 '_GW79tPHmVA',
 'NYE1GhhiF7E',
 'j1nwW8eyD0Q',
 '2S1XmMzFUe0',
 'WAYBZTQoVkI',
 'veSNESx8XBk',
 'jA8nVFz94eY',
 'A41Xtv_tatE',
 'kufA4JGm_sU',
 '0pR_tZAVnUE',
 '1_kJsHwf64c',
 '2Rq34fbl1J4',
 'Ksw0yqOK00I',
 'v3smSegJR50',
 'MyH8ItOpkjA',
 '-RHTuM3hNII',
 'UVMtTO5H1bk',
 'ejyLvEIpv-Q',
 'yI1vvwPKB1c',
 'h8JY9daqCBY',
 'kqzOGLstWh8',
 'SucuT1pw8Jc',
 'QBrcu1kBd7k',
 'lr9ToJnwPEk',
 'E2g4UffqKw0'
]

# Inicializando el API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Extrayendo los comentarios (incluidas respuestas) del video
def get_comments_for_video(youtube, video_id):
    all_comments = []
    next_page_token = None

    while True:
        try:
            # Convocando el API para extraer los comentarios
            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100
            ).execute()

            # Navegando en cada comentario y añadiendo a una lista
            for item in response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'Timestamp': top_comment['publishedAt'],
                    'Username': top_comment['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': top_comment['textDisplay'],
                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt'],
                    'Likes': top_comment['likeCount'],
                    'Replies': item['snippet']['totalReplyCount']
                }
                all_comments.append(comment_data)

                # Obteniendo respuestas (si existe algunas)
                if item['snippet']['totalReplyCount'] > 0:
                    all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return all_comments

# Extrayendo las respuestas a los comentarios
def get_replies(youtube, parent_id, video_id):
    replies = []
    next_page_token = None

    while True:
        try:
            # Llamando el API para obtener respuestas
            response = youtube.comments().list(
                part='snippet',
                parentId=parent_id,
                textFormat='plainText',
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada respuesta y añadiendo a una lista
            for item in response['items']:
                reply_data = {
                    'Timestamp': item['snippet']['publishedAt'],
                    'Username': item['snippet']['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': item['snippet']['textDisplay'],
                    'Date': item['snippet']['updatedAt'] if 'updatedAt' in item['snippet'] else item['snippet']['publishedAt'],
                    'Likes': item['snippet']['likeCount'],
                    'Replies': 0  # Las respuestas a respuestas no se toman en cuenta como etiquetas (no están etiquetados)
                }
                replies.append(reply_data)

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return replies

# Creando un Dataframe con todos los comentarios de algún video
def create_comments_dataframe(video_id, comments):
    df = pd.DataFrame(comments)
    df['VideoId'] = video_id
    df['VideoUrl'] = f'https://www.youtube.com/watch?v={video_id}'
    return df

# Función principal
def main():

    # Inicializando el API de YouTube
    youtube = initialize_youtube()

    # Listando los comentarios en Dataframes
    all_dfs = []

    # Navegando en cada ID del video y extrayendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_comments_for_video(youtube, video_id)
        df = create_comments_dataframe(video_id, comments)
        all_dfs.append(df)

    # Concatenando los Dataframes en un único DF
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Guardando el dataframe en un archivo CSV
    final_df.to_csv('youtube_comments_with_likes_replies.csv', index=False)

    print('Proceso completado. Se ha creado el archivo youtube_comments_with_likes_replies.csv')

if __name__ == '__main__':
    main()

from google.colab import files

# Activando una descarga en nuestro procesador
files.download("youtube_comments_with_likes_replies.csv")

"""$\small\text{7. Creación de un Dataframe de los comentarios de los usuarios y gráfico de un diagrama de barras (indicando la frecuencia del top 10)}$"""

import pandas as pd
import matplotlib.pyplot as plt
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Configurando la clave del API Key en la consola para desarrolladores de Google
API_KEY = 'AIzaSyBdU3XsjhpgZtLLFKoqp9EUHeNI-7U7J30'

# Listando de ID's de video de YouTube
video_ids = [
 'rd2jKGQJucE',
 '_GW79tPHmVA',
 'NYE1GhhiF7E',
 'j1nwW8eyD0Q',
 '2S1XmMzFUe0',
 'WAYBZTQoVkI',
 'veSNESx8XBk',
 'jA8nVFz94eY',
 'A41Xtv_tatE',
 'kufA4JGm_sU',
 '0pR_tZAVnUE',
 '1_kJsHwf64c',
 '2Rq34fbl1J4',
 'Ksw0yqOK00I',
 'v3smSegJR50',
 'MyH8ItOpkjA',
 '-RHTuM3hNII',
 'UVMtTO5H1bk',
 'ejyLvEIpv-Q',
 'yI1vvwPKB1c',
 'h8JY9daqCBY',
 'kqzOGLstWh8',
 'SucuT1pw8Jc',
 'QBrcu1kBd7k',
 'lr9ToJnwPEk',
 'E2g4UffqKw0'
]

# Inicializando el API de YouTube
def initialize_youtube():
    youtube = build('youtube', 'v3', developerKey=API_KEY)
    return youtube

# Obteniendo los comentarios (incluidas respuestas)
def get_comments_for_video(youtube, video_id):
    all_comments = []
    next_page_token = None

    while True:
        try:
            # Convocando el API para extraer los comentarios
            response = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100
            ).execute()

            # Navegando en cada comentario y añadiendo a una lista
            for item in response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                comment_data = {
                    'Timestamp': top_comment['publishedAt'],
                    'Username': top_comment['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': top_comment['textDisplay'],
                    'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt'],
                    'Likes': top_comment['likeCount'],
                    'Replies': item['snippet']['totalReplyCount']
                }
                all_comments.append(comment_data)

                # Obteniendo las respuestas (si existe alguna)
                if item['snippet']['totalReplyCount'] > 0:
                    all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return all_comments

# Obteniendo las respuestas a los comentarios
def get_replies(youtube, parent_id, video_id):
    replies = []
    next_page_token = None

    while True:
        try:
            # Emplazando el API para extraer las respuestas
            response = youtube.comments().list(
                part='snippet',
                parentId=parent_id,
                textFormat='plainText',
                maxResults=100,
                pageToken=next_page_token
            ).execute()

            # Navegando en cada respuesta y añadiendo a una lista
            for item in response['items']:
                reply_data = {
                    'Timestamp': item['snippet']['publishedAt'],
                    'Username': item['snippet']['authorDisplayName'],
                    'VideoID': video_id,
                    'Comment': item['snippet']['textDisplay'],
                    'Date': item['snippet']['updatedAt'] if 'updatedAt' in item['snippet'] else item['snippet']['publishedAt'],
                    'Likes': item['snippet']['likeCount'],
                    'Replies': 0  # Las respuestas a respuestas no se cuentan como etiquetas
                }
                replies.append(reply_data)

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

        except HttpError as e:
            print(f'Error en la llamada a la API: {e}')
            break

    return replies

# Obteniendo los comentarios de los usuarios para un gráfico de frecuencias (top 10)
def main():

    # Inicializando el API de YouTube
    youtube = initialize_youtube()

    # Listando los Dataframes de comentarios para su almacenamiento
    all_dfs = []

    # Navegando en cada ID de un video y extrayendo los comentarios
    for video_id in video_ids:
        print(f'Obteniendo comentarios para el video: {video_id}')
        comments = get_comments_for_video(youtube, video_id)
        df = pd.DataFrame(comments)
        all_dfs.append(df)

    # Concatenando todos los Dataframes en un único DF
    final_df = pd.concat(all_dfs, ignore_index=True)

    # Adquiriendo listado de los usuarios que comentaron
    user_counts = final_df['Username'].value_counts().head(10)

    # Creando el gráfico de barras
    plt.figure(figsize=(10, 6))
    user_counts.plot(kind='bar', color='red')
    plt.title('Top 10 Usuarios que Comentaron en los Videos')
    plt.xlabel('Usuarios')
    plt.ylabel('Frecuencia')
    plt.xticks(rotation=90, ha='right')
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()

"""$\small\text{8. Análisis de Sensaciones}$"""

!pip install textblob

!ls

import pandas as pd
from textblob import TextBlob
import seaborn as sns

# Importando el archivo de sensaciones desde GitHub
url = 'https://raw.githubusercontent.com/EHN8829/DUPD_FINAL/main/youtube_comments.csv'
df = pd.read_csv(url)

# Mostrando las primeras filas del DataFrame
print(df.head(25))

df.head(50)

df['polaridad']=df['Comment'].apply(lambda x: TextBlob(x).sentiment.polarity)
df['subjetividad']=df['Comment'].apply(lambda x: TextBlob(x).sentiment.subjectivity)

sns.displot(df['polaridad'],color='navy')
sns.displot(df['subjetividad'],color='lightblue')

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Creando un gráfico superpuesto
plt.figure(figsize=(10, 6))  # Ajuste del tamaño del gráfico (según el caso)

# Graficando la polaridad en rojo oscuro
sns.histplot(df['polaridad'], color='crimson', kde=True, label='Polaridad')

# Graficando la subjetividad en verde claro
sns.histplot(df['subjetividad'], color='cornsilk', kde=True, label='Subjetividad')

# Ajustando las etiquetas y leyenda
plt.xlabel('Valor')
plt.ylabel('Frecuencia')
plt.title('Distribución de Polaridad y Subjetividad')
plt.legend()

# Mostrando el gráfico
plt.show()

"""$\text{- Se observa que los valores para la subjetividad [cornsilk] y polaridad [crimson] son mayores a cero, respectivamente.}$
$\text{- Significa que los comentarios son objetivos respecto a la subjetividad y respecto a la polaridad existe comentarios positivos y negativos.}$
$\text{- Predomina una mayor cantidad de comentarios neutros, como una mínima cantidad de positivos. Pero, mayor respecto a los comentarios negativos.}$

$\small\text{9. Adicional: Nube de tags (palabras más significativas)}$
"""

from wordcloud import WordCloud

text = ' '.join(df['Comment'])
text

wordcloud = WordCloud(max_font_size=60, max_words=200, background_color="azure").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")

wordcloud = WordCloud(width=1024, height=800, colormap="Reds", min_font_size=14).generate(text)

plt.figure(figsize=(12, 10), facecolor=None)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()